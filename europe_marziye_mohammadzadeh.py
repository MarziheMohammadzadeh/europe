# -*- coding: utf-8 -*-
"""Europe_Marziye_Mohammadzadeh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BqYKnlXGc_JudgF6i0sSt0lMEYZclMsU
"""

import pandas as pd

# Load the dataset
file_path = '/content/GCC_PlanningRegisterSites_16.csv'
dataset = pd.read_csv(file_path)

dataset.info()
print(dataset.head())

date_columns = ['ReceivedDate', 'DecisionDate', 'DecisionDueDate',
                'WithdrawnDate', 'GrantDate', 'ExpiryDate',
                'AppealNotificationDate', 'AppealDecisionDate']

for col in date_columns:
    dataset[col] = pd.to_datetime(dataset[col], errors='coerce')

print(dataset.describe())

print(dataset.isnull().sum())

dataset.fillna('n/a', inplace=True)

galway_data = dataset[dataset['County'] == 'Galway']

galway_cc_data = dataset[dataset['PlanningAuthority'] == 'Galway County Council']

status_counts = dataset['ApplicationStatus'].value_counts()
print(status_counts)

import matplotlib.pyplot as plt

status_counts.plot(kind='bar')
plt.title('Distribution of Application Statuses')
plt.xlabel('Application Status')
plt.ylabel('Count')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

file_path = '/content/GCC_PlanningRegisterSites_16.csv'
dataset = pd.read_csv(file_path)

dataset['ReceivedDate'] = pd.to_datetime(dataset['ReceivedDate'], format='%d/%m/%Y', errors='coerce')

dataset['YearReceived'] = dataset['ReceivedDate'].dt.year

applications_per_year = dataset['YearReceived'].value_counts().sort_index()

applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

import matplotlib.pyplot as plt

# Plot the number of applications received per year
applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

dataset['YearReceived'] = dataset['ReceivedDate'].dt.year

applications_per_year = dataset['YearReceived'].value_counts().sort_index()

applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

output_path = '/content/GCC_PlanningRegisterSites_16.csv'
dataset.to_csv(output_path, index=False)

missing_values = dataset.isnull().sum()
print(missing_values[missing_values > 0])

dataset.fillna('n/a', inplace=True)

date_columns = ['ReceivedDate', 'DecisionDate', 'DecisionDueDate',
                'WithdrawnDate', 'GrantDate', 'ExpiryDate',
                'AppealNotificationDate', 'AppealDecisionDate']

for col in date_columns:
    dataset[col] = pd.to_datetime(dataset[col], format='%d/%m/%Y', errors='coerce')

print(dataset.describe())
print(dataset.describe(include=['object']))

dataset['Shape__Area'].plot(kind='hist', bins=50)
plt.title('Distribution of Site Area')
plt.xlabel('Area')
plt.show()
dataset['ApplicationType'].value_counts().plot(kind='bar')
plt.title('Frequency of Application Types')
plt.xlabel('Application Type')
plt.ylabel('Count')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
numerical_data = dataset.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

dataset['DecisionDuration'] = (dataset['DecisionDate'] - dataset['ReceivedDate']).dt.days

dataset['AppealMade'] = dataset['AppealRefNum'].notnull().astype(int)

dataset = pd.get_dummies(dataset, columns=['County', 'PlanningAuthority', 'ApplicationType'])

from sklearn.model_selection import train_test_split
X = dataset.drop(columns=['DecisionDuration', 'OBJECTID', 'ApplicantName', 'ApplicationNumber'])
y = dataset['DecisionDuration']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(y_train.isnull().sum())

y_train = y_train.dropna()
X_train = X_train.loc[y_train.index]

y_train.fillna(y_train.mean(), inplace=True)

from sklearn.ensemble import RandomForestRegressor

if 'datetime_column' in X_train.columns:
    X_train['datetime_column'] = X_train['datetime_column'].astype(int) / 10**9
else:
    print("Column 'datetime_column' not found in X_train DataFrame.")

if 'ReceivedDate' in X_train.columns:
    X_train['year'] = X_train['ReceivedDate'].dt.year
    X_train['month'] = X_train['ReceivedDate'].dt.month
    X_train['day'] = X_train['ReceivedDate'].dt.day
else:
    print("Column 'ReceivedDate' not found in X_train DataFrame.")

from sklearn.ensemble import RandomForestRegressor
import pandas as pd

for col in X_train.columns:
    if X_train[col].dtype == 'datetime64[ns]':
        X_train[col] = X_train[col].astype(int) / 10**9
    elif X_train[col].dtype == 'object':
        try:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')
        except:
            print(f"Could not convert column '{col}' to numeric. Consider encoding or dropping.")
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_imputed, y_train)

"""**in File W3 , We have " Properly comment on the birthday.py module. A template could be as follows; replace
the placeholder sentences with meaningful content " SO i want use that in my code :**
"""

def preprocess_data(dataset):
    '''
    This function preprocesses the dataset by handling missing values, encoding categorical variables,
    and splitting the dataset into features and target variables.

    Args:
    - dataset (pd.DataFrame): The original dataset.

    Returns:
    - X (pd.DataFrame): The features of the dataset.
    - y (pd.Series): The target variable.
    '''
    dataset = dataset.dropna()

    target_column_name = 'ActualTargetColumnName'

    if target_column_name in dataset.columns:
        y = dataset[target_column_name]
        X = dataset.drop(target_column_name, axis=1)

        X = pd.get_dummies(X, drop_first=True)

        return X, y
    else:
        print(f"Target column '{target_column_name}' not found in the dataset.")
        return None, None

'''
Module Goal:
This module processes a dataset, trains a machine learning model using a RandomForestRegressor, and evaluates its performance.
It handles tasks such as data loading, preprocessing, model training, and generating performance metrics.
'''

'''
Module Goal:
This module handles data loading, preprocessing, and provides utility functions for data analysis and visualization.
It is intended for use in data science projects where these tasks are required.
'''

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

print(os.getcwd())

import os
print(os.getcwd())
print(os.listdir())

pip install data_processing

import data_processing

import sys
sys.path.append('/content/GCC_PlanningRegisterSites_16.csv')
import data_processing

sys.path.append(os.path.join(current_dir, 'subdirectory'))
import data_processing

import sys
import os
if hasattr(sys, 'ps1'):
    current_dir = os.getcwd()
else:
    current_dir = os.path.dirname(os.path.abspath(__file__))

sys.path.append(current_dir)

try:
    import data_processing
except ModuleNotFoundError as e:
    print(f"Module not found: {e}")