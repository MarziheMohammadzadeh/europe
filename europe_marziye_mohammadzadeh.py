# -*- coding: utf-8 -*-
"""Europe_Marziye_Mohammadzadeh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DoZQFKo9s2aeWvc0VrQjVD5Z8qg7lWEc
"""

import pandas as pd

# Load the dataset
file_path = '/content/GCC_PlanningRegisterSites_16.csv'
dataset = pd.read_csv(file_path)

dataset.info()
print(dataset.head())

date_columns = ['ReceivedDate', 'DecisionDate', 'DecisionDueDate',
                'WithdrawnDate', 'GrantDate', 'ExpiryDate',
                'AppealNotificationDate', 'AppealDecisionDate']

for col in date_columns:
    dataset[col] = pd.to_datetime(dataset[col], errors='coerce')

print(dataset.describe())

print(dataset.isnull().sum())

dataset.fillna('n/a', inplace=True)

galway_data = dataset[dataset['County'] == 'Galway']

galway_cc_data = dataset[dataset['PlanningAuthority'] == 'Galway County Council']

status_counts = dataset['ApplicationStatus'].value_counts()
print(status_counts)

import matplotlib.pyplot as plt

status_counts.plot(kind='bar')
plt.title('Distribution of Application Statuses')
plt.xlabel('Application Status')
plt.ylabel('Count')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

file_path = '/content/GCC_PlanningRegisterSites_16.csv'
dataset = pd.read_csv(file_path)

dataset['ReceivedDate'] = pd.to_datetime(dataset['ReceivedDate'], format='%d/%m/%Y', errors='coerce')

dataset['YearReceived'] = dataset['ReceivedDate'].dt.year

applications_per_year = dataset['YearReceived'].value_counts().sort_index()

applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

import matplotlib.pyplot as plt

# Plot the number of applications received per year
applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

dataset['YearReceived'] = dataset['ReceivedDate'].dt.year

applications_per_year = dataset['YearReceived'].value_counts().sort_index()

applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

output_path = '/content/GCC_PlanningRegisterSites_16.csv'
dataset.to_csv(output_path, index=False)

missing_values = dataset.isnull().sum()
print(missing_values[missing_values > 0])

dataset.fillna('n/a', inplace=True)

date_columns = ['ReceivedDate', 'DecisionDate', 'DecisionDueDate',
                'WithdrawnDate', 'GrantDate', 'ExpiryDate',
                'AppealNotificationDate', 'AppealDecisionDate']

for col in date_columns:
    dataset[col] = pd.to_datetime(dataset[col], format='%d/%m/%Y', errors='coerce')

print(dataset.describe())
print(dataset.describe(include=['object']))

dataset['Shape__Area'].plot(kind='hist', bins=50)
plt.title('Distribution of Site Area')
plt.xlabel('Area')
plt.show()
dataset['ApplicationType'].value_counts().plot(kind='bar')
plt.title('Frequency of Application Types')
plt.xlabel('Application Type')
plt.ylabel('Count')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
numerical_data = dataset.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

dataset['DecisionDuration'] = (dataset['DecisionDate'] - dataset['ReceivedDate']).dt.days

dataset['AppealMade'] = dataset['AppealRefNum'].notnull().astype(int)

dataset = pd.get_dummies(dataset, columns=['County', 'PlanningAuthority', 'ApplicationType'])

from sklearn.model_selection import train_test_split
X = dataset.drop(columns=['DecisionDuration', 'OBJECTID', 'ApplicantName', 'ApplicationNumber'])
y = dataset['DecisionDuration']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(y_train.isnull().sum())

y_train = y_train.dropna()
X_train = X_train.loc[y_train.index]

y_train.fillna(y_train.mean(), inplace=True)

from sklearn.ensemble import RandomForestRegressor

if 'datetime_column' in X_train.columns:
    X_train['datetime_column'] = X_train['datetime_column'].astype(int) / 10**9
else:
    print("Column 'datetime_column' not found in X_train DataFrame.")

if 'ReceivedDate' in X_train.columns:
    X_train['year'] = X_train['ReceivedDate'].dt.year
    X_train['month'] = X_train['ReceivedDate'].dt.month
    X_train['day'] = X_train['ReceivedDate'].dt.day
else:
    print("Column 'ReceivedDate' not found in X_train DataFrame.")

from sklearn.ensemble import RandomForestRegressor
import pandas as pd

for col in X_train.columns:
    if X_train[col].dtype == 'datetime64[ns]':
        X_train[col] = X_train[col].astype(int) / 10**9
    elif X_train[col].dtype == 'object':
        try:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')
        except:
            print(f"Could not convert column '{col}' to numeric. Consider encoding or dropping.")
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_imputed, y_train)

"""**in File W3 , We have " Properly comment on the birthday.py module. A template could be as follows; replace
the placeholder sentences with meaningful content " SO i want use that in my code :**
"""

def preprocess_data(dataset):
    '''
    This function preprocesses the dataset by handling missing values, encoding categorical variables,
    and splitting the dataset into features and target variables.

    Args:
    - dataset (pd.DataFrame): The original dataset.

    Returns:
    - X (pd.DataFrame): The features of the dataset.
    - y (pd.Series): The target variable.
    '''
    dataset = dataset.dropna()

    target_column_name = 'ActualTargetColumnName'

    if target_column_name in dataset.columns:
        y = dataset[target_column_name]
        X = dataset.drop(target_column_name, axis=1)

        X = pd.get_dummies(X, drop_first=True)

        return X, y
    else:
        print(f"Target column '{target_column_name}' not found in the dataset.")
        return None, None

'''
Module Goal:
This module processes a dataset, trains a machine learning model using a RandomForestRegressor, and evaluates its performance.
It handles tasks such as data loading, preprocessing, model training, and generating performance metrics.
'''

'''
Module Goal:
This module handles data loading, preprocessing, and provides utility functions for data analysis and visualization.
It is intended for use in data science projects where these tasks are required.
'''

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

print(os.getcwd())

import os
print(os.getcwd())
print(os.listdir())

pip install data_processing

import data_processing

import sys
sys.path.append('/content/GCC_PlanningRegisterSites_16.csv')
import data_processing

sys.path.append(os.path.join(current_dir, 'subdirectory'))
import data_processing

import sys
import os
if hasattr(sys, 'ps1'):
    current_dir = os.getcwd()
else:
    current_dir = os.path.dirname(os.path.abspath(__file__))

sys.path.append(current_dir)

try:
    import data_processing
except ModuleNotFoundError as e:
    print(f"Module not found: {e}")

"""**Tasks W4**"""

import pandas as pd

# Load the dataset
dataset = pd.read_csv('/content/GCC_PlanningRegisterSites_16.csv')

# Display the first few rows
print(dataset.head())

# Get basic statistics
print(dataset.describe())
print(dataset.info())

dataset.ffill(inplace=True)

columns_to_drop = ['UnnecessaryColumn1', 'UnnecessaryColumn2']
existing_columns_to_drop = [col for col in columns_to_drop if col in dataset.columns]

if existing_columns_to_drop:
    dataset = dataset.drop(columns=existing_columns_to_drop)

dataset.ffill(inplace=True)
dataset['ReceivedDate'] = pd.to_datetime(dataset['ReceivedDate'])

columns_to_drop = ['UnnecessaryColumn1', 'UnnecessaryColumn2']
existing_columns_to_drop = [col for col in columns_to_drop if col in dataset.columns]

if existing_columns_to_drop:
    dataset = dataset.drop(columns=existing_columns_to_drop)

import pandas as pd

dataset = pd.read_csv('/content/GCC_PlanningRegisterSites_16.csv')

print(dataset.head())

print(dataset.columns)
dataset = dataset.drop(columns=['ActualColumnName1', 'ActualColumnName2'])

dataset.fillna(method='ffill', inplace=True)

dataset['ReceivedDate'] = pd.to_datetime(dataset['ReceivedDate'])

import pandas as pd

dataset = pd.read_csv('/content/GCC_PlanningRegisterSites_16.csv')

print(dataset.head())

print("Available columns:", dataset.columns)

columns_to_drop = ['ActualColumnName1', 'ActualColumnName2']


existing_columns_to_drop = [col for col in columns_to_drop if col in dataset.columns]

if existing_columns_to_drop:
    dataset = dataset.drop(columns=existing_columns_to_drop)
else:
    print(f"Columns {columns_to_drop} not found in dataset. No columns were dropped.")

dataset.ffill(inplace=True)

dataset['ReceivedDate'] = pd.to_datetime(dataset['ReceivedDate'])

import matplotlib.pyplot as plt
dataset['YearReceived'] = dataset['ReceivedDate'].dt.year
applications_per_year = dataset['YearReceived'].value_counts().sort_index()

applications_per_year.plot(kind='line')
plt.title('Applications Received Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Applications')
plt.show()

import seaborn as sns
import pandas as pd

# Select only numeric columns for correlation
numeric_dataset = dataset.select_dtypes(include=['number'])

correlation_matrix = numeric_dataset.corr()

# Plot a heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Double-check the actual name of your target variable column in the dataset
target_variable_column = 'ActualTargetVariableName'  # Replace with the correct name

# Verify if the column exists in the dataset
if target_variable_column in dataset.columns:
    X = dataset.drop(columns=[target_variable_column])
    y = dataset[target_variable_column]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    print(f'Mean Squared Error: {mse}')
else:
    print(f"Target variable column '{target_variable_column}' not found in dataset.")

"""**Tasks of W5**"""

mkdir tests

import pandas as pd
import os

# Define the path to your dataset
CSV_FILE_PATH = '/content/GCC_PlanningRegisterSites_16.csv'

# Test 1: Smoke Test - Valid Input
def test_smoke_valid_input():
    dataset = pd.read_csv(CSV_FILE_PATH)
    assert not dataset.empty, "The dataset should not be empty."
    assert 'ReceivedDate' in dataset.columns, "The 'ReceivedDate' column should be present."

# Test 2: Error Entry Test - Invalid Column
def test_error_entry():
    try:
        dataset = pd.read_csv(CSV_FILE_PATH)
        dataset['NonExistentColumn']
    except KeyError:
        pass  # Expected behavior
    else:
        raise AssertionError("Accessing a non-existent column should raise a KeyError.")

# Test 3: Corner Case Test - Empty Dataset
def test_corner_case():
    empty_dataset = pd.DataFrame(columns=['ReceivedDate'])
    assert empty_dataset.empty, "The dataset should be empty."

import csv
import os
CSV_FILE_PATH = '/content/GCC_PlanningRegisterSites_16.csv'

def test_csv_file_exists():
    assert os.path.isfile(CSV_FILE_PATH), f"CSV file not found at: {CSV_FILE_PATH}"

def test_csv_file_format():
    with open(CSV_FILE_PATH, 'r', newline='') as csv_file:
        csv_reader = csv.reader(csv_file)
        header = next(csv_reader, None)  # Read the header row
        assert header is not None, "CSV file is empty"
        assert len(header) > 0, "CSV file should have at least one column"

"""*Do test*"""

!pytest --cov=app --cov-report=html tests/

# This setup provides a framework for testing various aspects of your dataset and application.
# The tests will help ensure that your data processing logic is robust and handles edge cases and errors appropriately.